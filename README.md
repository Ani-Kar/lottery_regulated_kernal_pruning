# Lottery Regulated Grouped Kernal Pruning

## Contributors
- Aniket Kumar
- Vishal Ravipati

## About
The goals of the this Project are as follows:
- Revisit the idea of kernel pruning (to only prune one or several k × k kernels from a
3D-filter, instead of an entire one) as an alternative and less aggressive pruning approach with higher
degree of freedom.
- Design a simple and cost-efficient greedy algorithm with multiple restarts to generate multiple 
candidate kernel selection queues and identify the
one queue where the preserved kernels are most “distinctive” from each other yet “similar” to the
pruned kernels.

## Main Advantages 
- Bring back an overlooked approach: Brought attention to the heavily overlooked
approach of kernel pruning under the context of densely structured pruning.
- Simple but effective — a vanilla adaptation of the framework outperforms sophisticated 
variants of other comparable frameworks.
- Better longevity: Developed a framework that is compatible with further-developed/discovered 
clustering schemes and inductive biases, or more advanced variations
upon them.
- Improved deployability: The resultant network of the method is structured as a densely
grouped convolution, which enables parallel computing capability and greatly increases the
practical deployability of our methods.

## Method
The overall procedure of our method can be mainly divided into four stages: 

1) Clustering filters into n equal-sized groups, where the best clustering scheme for each convolutional layer is determined
using the tickets magnitude increase score derived from prior arts on lottery ticket hypothesis and
weight-shifting.
2) Evaluating several candidate grouped kernel pruning strategies generated by a
greedy approximation algorithm with multiple restarts, where the strategy with preserved grouped
kernels that are most distinctive from each other, yet most similar to the pruned grouped kernels
gets selected.
3) Permuting the preserved filters to form a grouped convolutional architecture with n
groups.
4) Fine-tuning the pruned and grouped network to recover accuracy lost from pruning.

**Instructions:**
- Create the Conda Env using the environment.yml file provided [link](https://github.com/Ani-Kar/lottery_regulated_kernal_pruning/blob/main/environment.yml) and run<br /><br />
```
    python main.py 
        --exp_desc resnet20_cifar10_demo 
        --setting_dir settings/resnet20_cifar10_reported.json 
        --model_dir baseline/resnet20_cifar10_baseline 
        --snapshot_folder_dir snapshots/resnet20/ 
        --output_folder_dir output/resnet20_cifar10_demo 
        --baseline True 
        --task finetune
```
## References
- [Lottery_Regulated_Group_Kernal_Pruning](https://github.com/Ani-Kar/lottery_regulated_kernal_pruning/blob/main/2257_revisit_kernel_pruning_with_lo.pdf)




